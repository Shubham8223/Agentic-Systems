{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3 langchain_aws mypy_boto3_bedrock_runtime neo4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQaphhCUpnY1",
        "outputId": "af915eb7-306d-4409-8535-97148e61d496"
      },
      "id": "sQaphhCUpnY1",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (1.38.17)\n",
            "Requirement already satisfied: langchain_aws in /usr/local/lib/python3.11/dist-packages (0.2.23)\n",
            "Requirement already satisfied: mypy_boto3_bedrock_runtime in /usr/local/lib/python3.11/dist-packages (1.38.4)\n",
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.11/dist-packages (5.28.1)\n",
            "Requirement already satisfied: botocore<1.39.0,>=1.38.17 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.38.17)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from boto3) (0.12.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain_aws) (0.3.59)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain_aws) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from langchain_aws) (2.11.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from mypy_boto3_bedrock_runtime) (4.13.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from neo4j) (2025.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.17->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.17->boto3) (2.4.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.59->langchain_aws) (0.3.42)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.59->langchain_aws) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.59->langchain_aws) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.59->langchain_aws) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.59->langchain_aws) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.10.0->langchain_aws) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.10.0->langchain_aws) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.10.0->langchain_aws) (0.4.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.59->langchain_aws) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.59->langchain_aws) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.59->langchain_aws) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.59->langchain_aws) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.59->langchain_aws) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.59->langchain_aws) (0.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.17->boto3) (1.17.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.59->langchain_aws) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.59->langchain_aws) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.59->langchain_aws) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.59->langchain_aws) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.59->langchain_aws) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.59->langchain_aws) (3.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.59->langchain_aws) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy\n",
        "!pip install numpy==1.23.5 --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "lCKzcc-qIR3e",
        "outputId": "d87b4655-d643-4578-9edf-11ad152a8ba2"
      },
      "id": "lCKzcc-qIR3e",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.23.5\n",
            "Uninstalling numpy-1.23.5:\n",
            "  Would remove:\n",
            "    /usr/local/bin/f2py\n",
            "    /usr/local/bin/f2py3\n",
            "    /usr/local/bin/f2py3.11\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy-1.23.5.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libgfortran-040039e1.so.5.0.0\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libquadmath-96973f99.so.0.0.0\n",
            "    /usr/local/lib/python3.11/dist-packages/numpy/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled numpy-1.23.5\n",
            "Collecting numpy==1.23.5\n",
            "  Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.4.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "bdbebec8e8024ceba38319cc2dae9b2d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from enum import Enum\n",
        "\n",
        "class ModelId(str, Enum):\n",
        "    CLAUDE_3_HAIKU = \"us.anthropic.claude-3-haiku-20240307-v1:0\"\n",
        "    CLAUDE_3_SONNET = \"us.anthropic.claude-3-sonnet-20240229-v1:0\"\n",
        "    CLAUDE_3_OPUS = \"us.anthropic.claude-3-opus-20240229-v1:0\",\n",
        "    CLAUDE_3_5_SONNET =\"us.anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
        "    LLAMA_3_3_70B =\"us.meta.llama3-3-70b-instruct-v1:0\"\n",
        "\n",
        "class ModelKwargsClaude(BaseModel):\n",
        "    temperature: float = Field(default=0.1, ge=0, le=1)\n",
        "    max_tokens: int = Field(default=2048, ge=1, le=4096)\n",
        "    top_p: float = Field(default=0.999, ge=0, le=1)\n",
        "    top_k: int = Field(default=0, ge=0, le=500)\n",
        "\n",
        "class ModelKwargsLlama(BaseModel):\n",
        "    temperature: float = Field(default=0.1, ge=0, le=1)\n",
        "    max_tokens: int = Field(default=2048, ge=1, le=4096)\n",
        "    top_p: float = Field(default=0.999, ge=0, le=1)"
      ],
      "metadata": {
        "id": "SdWvrPaNpaCI"
      },
      "id": "SdWvrPaNpaCI",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "from langchain_aws.chat_models import ChatBedrock\n",
        "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
        "from mypy_boto3_bedrock_runtime.client import BedrockRuntimeClient\n",
        "from typing import Union, List\n",
        "aws_creds = {\n",
        "    \"access_key\": \"YOUR_ACCESS_KEY\",\n",
        "    \"secret_key\": \"YOUR_SECRET_KEY\",\n",
        "    \"region\": \"us-east-1\"\n",
        "}\n",
        "def get_bedrock_client():\n",
        "    session = boto3.Session(\n",
        "        aws_access_key_id=aws_creds[\"access_key\"],\n",
        "        aws_secret_access_key=aws_creds[\"secret_key\"],\n",
        "        region_name=aws_creds[\"region\"]\n",
        "    )\n",
        "    client = session.client(\"bedrock-runtime\")\n",
        "    return client\n",
        "\n",
        "def get_bedrock_model(\n",
        "  client: BedrockRuntimeClient,\n",
        "  model_id: ModelId,\n",
        "  model_kwargs: Union[ModelKwargsClaude,ModelKwargsLlama],\n",
        "  streaming: bool = False,\n",
        "  verbose: bool = False) -> ChatBedrock:\n",
        "    return ChatBedrock(\n",
        "        client=client,\n",
        "        model_id=model_id.value,\n",
        "        model_kwargs=model_kwargs.__dict__,\n",
        "        streaming=streaming,\n",
        "        verbose=verbose,\n",
        "        callbacks=[StreamingStdOutCallbackHandler()] if streaming else []\n",
        "    )"
      ],
      "metadata": {
        "id": "56JyGsGvo-aM"
      },
      "id": "56JyGsGvo-aM",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Entity(BaseModel):\n",
        "    entity_name: str = Field(..., description=\"The capitalized name of the entity (e.g., 'Federated Learning')\")\n",
        "    entity_type: str = Field(..., description=\"The type or category of the entity (e.g., 'large language model', 'dataset')\")\n",
        "    entity_description: str = Field(..., description=\"A detailed description of the entity, explaining its attributes and activities\")\n",
        "\n",
        "class Relationship(BaseModel):\n",
        "    source_entity: str = Field(..., description=\"The name of the source entity in the relationship\")\n",
        "    target_entity: str = Field(..., description=\"The name of the target entity in the relationship\")\n",
        "    relationship_description: str = Field(..., description=\"A description explaining why and how the entities are related\")\n",
        "    relationship_strength: int = Field(..., ge=1, le=10, description=\"An integer score between 1 and 10 indicating the strength of the relationship\")\n",
        "\n",
        "class DocumentAnalysis(BaseModel):\n",
        "    entities: List[Entity] = Field(..., description=\"A list of identified entities with their names, types, and descriptions\")\n",
        "    relationships: List[Relationship] = Field(..., description=\"A list of relationships between the identified entities, including source and target entities, relationship description, and relationship strength\")\n"
      ],
      "metadata": {
        "id": "Nh6UFMz1r36V"
      },
      "id": "Nh6UFMz1r36V",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import asyncio\n",
        "from typing import List, Dict, Any, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from neo4j import GraphDatabase\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Entity(BaseModel):\n",
        "    entity_name: str = Field(description=\"Name of the entity (capitalized)\")\n",
        "    entity_type: str = Field(description=\"Type of the entity from predefined categories\")\n",
        "    entity_description: str = Field(description=\"Detailed description of the entity\")\n",
        "\n",
        "class Relationship(BaseModel):\n",
        "    source_entity: str = Field(description=\"Name of the source entity\")\n",
        "    target_entity: str = Field(description=\"Name of the target entity\")\n",
        "    relationship_description: str = Field(description=\"Description of the relationship\")\n",
        "    relationship_strength: int = Field(description=\"Strength of relationship (1-10)\")\n",
        "\n",
        "class DocumentAnalysis(BaseModel):\n",
        "    entities: List[Entity] = Field(description=\"List of extracted entities\")\n",
        "    relationships: List[Relationship] = Field(description=\"List of relationships between entities\")\n",
        "\n",
        "class KnowledgeGraph:\n",
        "    def __init__(self, client, claude_model, neo4j_uri=\"your_db\",\n",
        "                 neo4j_user=\"neo4j\", neo4j_password=\"password\"):\n",
        "        self.client = client\n",
        "        self.claude_model = claude_model\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=4000,\n",
        "            chunk_overlap=200,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
        "        self.session = self.driver.session()\n",
        "\n",
        "        self._initialize_database()\n",
        "\n",
        "        self.all_entities = {}\n",
        "        self.all_relationships = []\n",
        "\n",
        "    def _initialize_database(self):\n",
        "        \"\"\"Create constraints and indexes for better performance\"\"\"\n",
        "        try:\n",
        "            self.session.run(\"CREATE CONSTRAINT entity_name_unique IF NOT EXISTS FOR (e:Entity) REQUIRE e.name IS UNIQUE\")\n",
        "\n",
        "            self.session.run(\"CREATE INDEX entity_type_index IF NOT EXISTS FOR (e:Entity) ON (e.type)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Database initialization warning: {e}\")\n",
        "\n",
        "    async def extract_entities_and_relationships(self, texts):\n",
        "        \"\"\"Extract entities and relationships from a text chunk\"\"\"\n",
        "        parser = JsonOutputParser(pydantic_object=DocumentAnalysis)\n",
        "\n",
        "        messages = [\n",
        "            (\"system\", \"\"\"You are an expert assistant specializing in analyzing textual insights and extracting business-related topics, entities, and relationships.\n",
        "            You excel at identifying technical concepts, tools, methodologies, and their interconnections in complex documents.\"\"\"),\n",
        "            (\"human\", \"\"\"\n",
        "            Please analyze the following text and perform the following tasks:\n",
        "\n",
        "            1. Identify all entities from the provided text. For each identified entity, extract:\n",
        "                - entity_name: Name of the entity (capitalize properly)\n",
        "                - entity_type: Determine the most appropriate type based on context (e.g., \"Technology\", \"Methodology\", \"Framework\", \"Tool\", \"Algorithm\", \"Concept\", \"Person\", \"Organization\", \"Standard\", \"Protocol\", etc.)\n",
        "                - entity_description: Provide a detailed description (2-3 sentences)\n",
        "\n",
        "            2. For each pair of related entities, identify relationships:\n",
        "                - source_entity: Name of the source entity\n",
        "                - target_entity: Name of the target entity\n",
        "                - relationship_description: Clear description of the relationship\n",
        "                - relationship_strength: Rate 1-10 (10 being strongest)\n",
        "\n",
        "            3. Focus on meaningful entities and relationships, not trivial ones.\n",
        "            4. Use your knowledge to determine the most appropriate entity type for each entity based on its context and meaning.\n",
        "            5. Return results in the specified JSON format.\n",
        "\n",
        "            Text to analyze:\n",
        "            '''{texts}'''\n",
        "\n",
        "            {format_instructions}\"\"\")\n",
        "        ]\n",
        "\n",
        "        chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
        "        chain = chat_prompt | self.claude_model | parser\n",
        "\n",
        "        try:\n",
        "            response = await chain.ainvoke({\n",
        "                \"texts\": texts,\n",
        "                \"format_instructions\": parser.get_format_instructions()\n",
        "            })\n",
        "\n",
        "            if isinstance(response, str):\n",
        "                response = json.loads(response)\n",
        "\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting entities: {e}\")\n",
        "            return {\"entities\": [], \"relationships\": []}\n",
        "\n",
        "    def merge_entities(self, new_entities):\n",
        "        \"\"\"Merge new entities with existing ones, handling duplicates\"\"\"\n",
        "        for entity in new_entities:\n",
        "            entity_name = entity[\"entity_name\"].strip().title()\n",
        "\n",
        "            query = \"\"\"\n",
        "            MATCH (e:Entity {name: $name})\n",
        "            RETURN e\n",
        "            \"\"\"\n",
        "            result = self.session.run(query, name=entity_name)\n",
        "\n",
        "            if result.single():\n",
        "                update_query = \"\"\"\n",
        "                MATCH (e:Entity {name: $name})\n",
        "                SET e.description = e.description + ' ' + $new_description\n",
        "                \"\"\"\n",
        "                self.session.run(update_query, name=entity_name, new_description=entity['entity_description'])\n",
        "            else:\n",
        "                create_query = \"\"\"\n",
        "                CREATE (e:Entity {\n",
        "                    name: $name,\n",
        "                    type: $type,\n",
        "                    description: $description\n",
        "                })\n",
        "                \"\"\"\n",
        "                self.session.run(create_query,\n",
        "                               name=entity_name,\n",
        "                               type=entity.get('entity_type', ''),\n",
        "                               description=entity.get('entity_description', ''))\n",
        "\n",
        "            self.all_entities[entity_name] = entity\n",
        "\n",
        "    def merge_relationships(self, new_relationships):\n",
        "        \"\"\"Merge new relationships with existing ones\"\"\"\n",
        "        for rel in new_relationships:\n",
        "            source = rel[\"source_entity\"].strip().title()\n",
        "            target = rel[\"target_entity\"].strip().title()\n",
        "\n",
        "\n",
        "            check_query = \"\"\"\n",
        "            MATCH (s:Entity {name: $source})-[r:RELATED_TO]-(t:Entity {name: $target})\n",
        "            RETURN r\n",
        "            \"\"\"\n",
        "            result = self.session.run(check_query, source=source, target=target)\n",
        "\n",
        "            if result.single():\n",
        "\n",
        "                update_query = \"\"\"\n",
        "                MATCH (s:Entity {name: $source})-[r:RELATED_TO]-(t:Entity {name: $target})\n",
        "                SET r.strength = CASE WHEN r.strength < $strength THEN $strength ELSE r.strength END,\n",
        "                    r.description = r.description + ' ' + $description\n",
        "                \"\"\"\n",
        "                self.session.run(update_query,\n",
        "                               source=source,\n",
        "                               target=target,\n",
        "                               strength=rel[\"relationship_strength\"],\n",
        "                               description=rel[\"relationship_description\"])\n",
        "            else:\n",
        "                create_query = \"\"\"\n",
        "                MATCH (s:Entity {name: $source}), (t:Entity {name: $target})\n",
        "                CREATE (s)-[r:RELATED_TO {\n",
        "                    strength: $strength,\n",
        "                    description: $description\n",
        "                }]->(t)\n",
        "                \"\"\"\n",
        "                self.session.run(create_query,\n",
        "                               source=source,\n",
        "                               target=target,\n",
        "                               strength=rel[\"relationship_strength\"],\n",
        "                               description=rel[\"relationship_description\"])\n",
        "\n",
        "            self.all_relationships.append(rel)\n",
        "\n",
        "    async def process_large_document(self, document_text: str, chunk_size: int = 4000):\n",
        "        \"\"\"Process a large document by splitting it into chunks\"\"\"\n",
        "        print(\"Splitting document into chunks...\")\n",
        "        chunks = self.text_splitter.split_text(document_text)\n",
        "        print(f\"Document split into {len(chunks)} chunks\")\n",
        "\n",
        "        batch_size = 5\n",
        "        for i in range(0, len(chunks), batch_size):\n",
        "            batch = chunks[i:i+batch_size]\n",
        "            print(f\"Processing batch {i//batch_size + 1}/{(len(chunks) + batch_size - 1)//batch_size}\")\n",
        "\n",
        "            tasks = [self.extract_entities_and_relationships(chunk) for chunk in batch]\n",
        "            results = await asyncio.gather(*tasks)\n",
        "\n",
        "            for result in results:\n",
        "                if result and \"entities\" in result and \"relationships\" in result:\n",
        "                    self.merge_entities(result[\"entities\"])\n",
        "                    self.merge_relationships(result[\"relationships\"])\n",
        "\n",
        "        print(f\"Extraction complete! Found {len(self.all_entities)} entities and {len(self.all_relationships)} relationships\")\n",
        "\n",
        "    def analyze_graph(self):\n",
        "        \"\"\"Analyze the knowledge graph using Neo4j queries\"\"\"\n",
        "        analysis = {}\n",
        "\n",
        "        result = self.session.run(\"MATCH (n:Entity) RETURN count(n) as total\")\n",
        "        analysis[\"total_entities\"] = result.single()[\"total\"]\n",
        "\n",
        "\n",
        "        result = self.session.run(\"MATCH ()-[r:RELATED_TO]->() RETURN count(r) as total\")\n",
        "        analysis[\"total_relationships\"] = result.single()[\"total\"]\n",
        "\n",
        "        query = \"\"\"\n",
        "        MATCH (e:Entity)\n",
        "        OPTIONAL MATCH (e)-[r]-(connected)\n",
        "        WITH e, count(connected) as degree\n",
        "        RETURN e.name as entity, e.type as type, degree\n",
        "        ORDER BY degree DESC\n",
        "        LIMIT 10\n",
        "        \"\"\"\n",
        "        result = self.session.run(query)\n",
        "        analysis[\"most_connected_entities\"] = [{\"entity\": record[\"entity\"],\n",
        "                                               \"type\": record[\"type\"],\n",
        "                                               \"degree\": record[\"degree\"]}\n",
        "                                              for record in result]\n",
        "\n",
        "        query = \"\"\"\n",
        "        MATCH (e:Entity)\n",
        "        RETURN e.type as type, count(e) as count\n",
        "        ORDER BY count DESC\n",
        "        \"\"\"\n",
        "        result = self.session.run(query)\n",
        "        analysis[\"entity_types_distribution\"] = {record[\"type\"]: record[\"count\"] for record in result}\n",
        "\n",
        "        query = \"\"\"\n",
        "        MATCH (e:Entity)\n",
        "        OPTIONAL MATCH (e)-[r]-(connected)\n",
        "        WITH e, count(connected) as degree\n",
        "        RETURN avg(degree) as avg_degree\n",
        "        \"\"\"\n",
        "        result = self.session.run(query)\n",
        "        analysis[\"average_degree\"] = result.single()[\"avg_degree\"]\n",
        "\n",
        "\n",
        "        return analysis\n",
        "\n",
        "\n",
        "    def export_graph(self):\n",
        "        \"\"\"Export the knowledge graph from Neo4j\"\"\"\n",
        "        entities_query = \"MATCH (e:Entity) RETURN e.name as name, e.type as type, e.description as description\"\n",
        "        entities_result = self.session.run(entities_query)\n",
        "        entities = {record[\"name\"]: {\"entity_type\": record[\"type\"],\n",
        "                                   \"entity_description\": record[\"description\"]}\n",
        "                   for record in entities_result}\n",
        "\n",
        "        relations_query = \"\"\"\n",
        "        MATCH (s:Entity)-[r:RELATED_TO]->(t:Entity)\n",
        "        RETURN s.name as source, t.name as target, r.strength as strength, r.description as description\n",
        "        \"\"\"\n",
        "        relations_result = self.session.run(relations_query)\n",
        "        relationships = [{\"source_entity\": record[\"source\"],\n",
        "                         \"target_entity\": record[\"target\"],\n",
        "                         \"relationship_strength\": record[\"strength\"],\n",
        "                         \"relationship_description\": record[\"description\"]}\n",
        "                        for record in relations_result]\n",
        "\n",
        "        return {\n",
        "            \"entities\": entities,\n",
        "            \"relationships\": relationships,\n",
        "            \"graph_analysis\": self.analyze_graph()\n",
        "        }\n",
        "\n",
        "    def search_entities(self, query: str, limit: int = 10):\n",
        "        \"\"\"Search for entities by name or description using Neo4j full-text search\"\"\"\n",
        "        try:\n",
        "            self.session.run(\"CALL db.index.fulltext.createNodeIndex('entitySearch', ['Entity'], ['name', 'description'])\")\n",
        "        except:\n",
        "            pass  #\n",
        "\n",
        "        search_query = \"\"\"\n",
        "        CALL db.index.fulltext.queryNodes('entitySearch', $query)\n",
        "        YIELD node, score\n",
        "        RETURN node.name as name, node.type as type, node.description as description\n",
        "        ORDER BY score DESC\n",
        "        LIMIT $limit\n",
        "        \"\"\"\n",
        "        try:\n",
        "            result = self.session.run(search_query, query=query, limit=limit)\n",
        "            return [{\"name\": record[\"name\"],\n",
        "                    \"type\": record[\"type\"],\n",
        "                    \"description\": record[\"description\"]}\n",
        "                   for record in result]\n",
        "        except:\n",
        "            fallback_query = \"\"\"\n",
        "            MATCH (e:Entity)\n",
        "            WHERE toLower(e.name) CONTAINS toLower($query)\n",
        "               OR toLower(e.description) CONTAINS toLower($query)\n",
        "            RETURN e.name as name, e.type as type, e.description as description\n",
        "            LIMIT $limit\n",
        "            \"\"\"\n",
        "            result = self.session.run(fallback_query, query=query, limit=limit)\n",
        "            return [{\"name\": record[\"name\"],\n",
        "                    \"type\": record[\"type\"],\n",
        "                    \"description\": record[\"description\"]}\n",
        "                   for record in result]\n",
        "\n",
        "    def get_entity_relationships(self, entity_name: str):\n",
        "        \"\"\"Get all relationships for a specific entity\"\"\"\n",
        "        query = \"\"\"\n",
        "        MATCH (e:Entity {name: $name})-[r:RELATED_TO]-(other:Entity)\n",
        "        RETURN other.name as related_entity, r.strength as strength, r.description as description\n",
        "        \"\"\"\n",
        "        result = self.session.run(query, name=entity_name.strip().title())\n",
        "        return [{\"related_entity\": record[\"related_entity\"],\n",
        "                \"relationship_strength\": record[\"strength\"],\n",
        "                \"relationship_description\": record[\"description\"]}\n",
        "               for record in result]\n",
        "\n",
        "    def detect_and_store_communities(self):\n",
        "      \"\"\"Run Leiden community detection and store results in the graph\"\"\"\n",
        "      graph_name = \"community_detection_graph\"\n",
        "      try:\n",
        "\n",
        "          with self.driver.session() as session:\n",
        "              session.run(\"CALL gds.aura.api.credentials()\")\n",
        "\n",
        "              try:\n",
        "                  session.run(f\"CALL gds.graph.drop('{graph_name}')\")\n",
        "              except:\n",
        "                  pass\n",
        "\n",
        "              create_graph_query = f\"\"\"\n",
        "              CALL gds.graph.project(\n",
        "                  '{graph_name}',\n",
        "                  'Entity',\n",
        "                  'RELATED_TO',\n",
        "                  {{\n",
        "                      relationshipProperties: 'strength'\n",
        "                  }}\n",
        "              )\n",
        "              \"\"\"\n",
        "              session.run(create_graph_query)\n",
        "\n",
        "\n",
        "              leiden_write_query = f\"\"\"\n",
        "              CALL gds.leiden.write('{graph_name}', {{\n",
        "                  relationshipWeightProperty: 'strength',\n",
        "                  writeProperty: 'communityId',\n",
        "                  randomSeed: 42,\n",
        "                  maxLevels: 10\n",
        "              }})\n",
        "              YIELD nodePropertiesWritten, modularity, ranLevels\n",
        "              \"\"\"\n",
        "              result = session.run(leiden_write_query)\n",
        "              stats = result.single()\n",
        "\n",
        "              session.run(f\"CALL gds.graph.drop('{graph_name}')\")\n",
        "\n",
        "              return {\n",
        "                  \"nodes_updated\": stats[\"nodePropertiesWritten\"],\n",
        "                  \"modularity\": stats[\"modularity\"],\n",
        "                  \"levels_run\": stats[\"ranLevels\"]\n",
        "              }\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Error in community detection: {e}\")\n",
        "          return None\n",
        "\n",
        "\n",
        "    def get_communities(self):\n",
        "        \"\"\"Get all communities and their members\"\"\"\n",
        "        query = \"\"\"\n",
        "        MATCH (e:Entity)\n",
        "        WHERE e.communityId IS NOT NULL\n",
        "        WITH e.communityId as communityId, collect(e) as members\n",
        "        RETURN communityId,\n",
        "                [member IN members | {name: member.name, type: member.type}] as members,\n",
        "                size(members) as size\n",
        "        ORDER BY size DESC\n",
        "        \"\"\"\n",
        "        result = self.session.run(query)\n",
        "\n",
        "        communities = {}\n",
        "        for record in result:\n",
        "            comm_id = f\"Community_{record['communityId']}\"\n",
        "            communities[comm_id] = {\n",
        "                \"members\": record[\"members\"],\n",
        "                \"size\": record[\"size\"]\n",
        "            }\n",
        "\n",
        "        return communities\n",
        "\n",
        "    def get_entity_community(self, entity_name: str):\n",
        "        \"\"\"Get the community ID for a specific entity\"\"\"\n",
        "        query = \"\"\"\n",
        "        MATCH (e:Entity {name: $name})\n",
        "        RETURN e.communityId as communityId\n",
        "        \"\"\"\n",
        "        result = self.session.run(query, name=entity_name.strip().title())\n",
        "        record = result.single()\n",
        "        return record[\"communityId\"] if record else None\n",
        "\n",
        "    def get_community_summary(self):\n",
        "        \"\"\"Get summary statistics about communities\"\"\"\n",
        "        query = \"\"\"\n",
        "        MATCH (e:Entity)\n",
        "        WHERE e.communityId IS NOT NULL\n",
        "        WITH e.communityId as communityId, count(e) as size\n",
        "        RETURN min(size) as min_size,\n",
        "                max(size) as max_size,\n",
        "                avg(size) as avg_size,\n",
        "                count(*) as total_communities\n",
        "        \"\"\"\n",
        "        result = self.session.run(query)\n",
        "        return dict(result.single())\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close Neo4j connection\"\"\"\n",
        "        self.session.close()\n",
        "        self.driver.close()\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Cleanup when object is destroyed\"\"\"\n",
        "        try:\n",
        "            self.close()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close Neo4j connection\"\"\"\n",
        "        self.session.close()\n",
        "        self.driver.close()\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Cleanup when object is destroyed\"\"\"\n",
        "        try:\n",
        "            self.close()\n",
        "        except:\n",
        "            pass"
      ],
      "metadata": {
        "id": "SlRYWzsrwxgB"
      },
      "id": "SlRYWzsrwxgB",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = get_bedrock_client()\n",
        "\n",
        "claude_model = get_bedrock_model(\n",
        "    client=client,\n",
        "    model_id=ModelId.CLAUDE_3_5_SONNET,\n",
        "    model_kwargs=ModelKwargsClaude(),\n",
        ")"
      ],
      "metadata": {
        "id": "0R9KnExpJGRZ"
      },
      "id": "0R9KnExpJGRZ",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_text = \"\"\"International Journal of Research In Computer Applications and Information Technology (IJRCAIT) Volume 7, Issue 2, July-December 2024, pp. 98-110, Article ID: IJRCAIT_07_02_007 Available online at https://iaeme.com/Home/issue/IJRCAIT?Volume=7&Issue=2 ISSN Print: 2348-0009; ISSN Online: 2347-5099; Journal ID: 0497-2547 Impact Factor (2024): 14.56 (Based on Google Scholar Citation) DOI: https://doi.org/10.5281/zenodo.13908615 © IAEME Publication GRAPHRAG AND ROLE OF GRAPH DATABASES IN ADVANCING AI Ravi Kiran Magham Osmania University, India ABSTRACT This article explores the synergy between GraphRAG and graph databases in advancing AI capabilities. Graph RAG, a novel approach to query-focused summarization that combines knowledge graph generation, retrieval-augmented generation (RAG), and query-focused summarization (QFS) to support human sensemaking over large text corpora. While traditional RAG methods excel at answering specific questions, they struggle with global queries about entire datasets. Graph RAG addresses this limitation by using an LLM to build a graph-based text index in two stages: first deriving an entity knowledge graph from source documents, then pre-generating community summaries for groups of related entities. Given a query, each community summary generates a partial response, which is then synthesized into a final answer.  editor@iaeme.com https://iaeme.com/Home/journal/IJRCAIT 98 Graphrag and Role of Graph Databases in Advancing AI Evaluations on datasets in the 1 million token range show that Graph RAG substantially improves the comprehensiveness and diversity of answers compared to naïve RAG baselines, while also demonstrating favorable performance against global text summarization approaches. The hierarchical nature of the graph index allows for efficient querying at different levels of granularity. This approach offers a scalable solution for global sensemaking tasks over large private document collections, with potential applications in scientific discovery, intelligence analysis, and other domains requiring complex information synthesis. Keywords: GraphRAG, Graph Databases, Artificial Intelligence, Knowledge Representation, Large Language Models Cite this Article: Ravi Kiran Magham. (2024). Graphrag and Role of Graph Databases in Advancing AI. International Journal of Research in Computer Applications and Information Technology (IJRCAIT), 7(2), 98-110.  https://iaeme.com/MasterAdmin/Journal_uploads/IJRCAIT/VOLUME_7_ISSUE_2/IJRCAIT_07_02_007.pdf I. INTRODUCTION The landscape of Artificial Intelligence (AI) is rapidly evolving, with recent advancements in large language models (LLMs) and retrieval-augmented generation (RAG) pushing the boundaries of what's possible in natural language processing (NLP) tasks. Concurrently, graph databases have emerged as a powerful tool for representing and querying complex, interconnected data. The fusion of these technologies – which we term GraphRAG – presents a promising frontier in AI research and applications. Large language models, such as those described by Brown et al. [1], have demonstrated remarkable few-shot learning capabilities, allowing them to adapt to new tasks with minimal examples. However, these models often struggle with knowledge-intensive tasks that require access to large, structured datasets. To address this limitation, Lewis et al. [2] introduced the concept of retrieval-augmented generation, which combines the generative power of language models with the ability to retrieve and incorporate external knowledge. While RAG has shown significant promise, there's still room for improvement, particularly in handling complex, interconnected information. This is where graph databases come into play. As explained by Robinson et al. [3], graph databases excel at managing and querying highly connected data, making them ideal for representing the intricate relationships often found in real-world knowledge bases. The concept of GraphRAG builds upon these foundations, leveraging the strengths of graph databases to enhance the retrieval and reasoning capabilities of RAG systems. By representing knowledge in a graph structure, we can capture nuanced relationships between entities and concepts, potentially leading to more accurate and contextually relevant information retrieval. Moreover, the integration of graph databases into AI systems opens up new possibilities for explainable AI. The explicit representation of relationships in a graph allows for easier tracing of reasoning paths, providing insights into how an AI system arrives at its conclusions. This transparency is crucial for building trust in AI systems, especially in high-stakes domains like healthcare or finance. As we delve deeper into the synergies between graph databases and advanced AI techniques like RAG, we stand at the cusp of a new era in artificial intelligence. GraphRAG has the potential to not only improve the performance of AI systems on knowledge-intensive tasks but also to enhance their interpretability and reliability. https://iaeme.com/Home/journal/IJRCAIT 99 editor@iaeme.com Ravi Kiran Magham In the following sections, we will explore the theoretical foundations of GraphRAG, discuss its potential applications, and examine the challenges and opportunities that lie ahead in this exciting field of research. Fig 1: Comparative Capabilities of AI Technologies in Knowledge-Intensive Tasks [1-3] II. GRAPHRAG: ENHANCING LLM REASONING GraphRAG represents a significant leap forward in leveraging the power of graph databases to enhance the reasoning capabilities of Large Language Models (LLMs). By seamlessly integrating structured knowledge from graphs with the generative prowess of LLMs, GraphRAG empowers AI systems to navigate complex relationships, answer intricate queries, and unlock deeper insights from vast datasets. At its core, GraphRAG operates through a series of interconnected stages, each designed to optimize the interaction between LLMs and graph-structured knowledge. Let's explore these key stages: 1. Graph-Based Indexing  The journey begins with the creation of a graph-based index, a structured representation of knowledge that facilitates efficient retrieval. This stage involves: Knowledge Graph Construction: • Leverage existing open-source knowledge graphs (e.g., Wikidata, Freebase, DBpedia) or domain-specific ones (e.g., CMeKG for biomedical data). • Construct custom knowledge graphs from textual or other data sources, extracting entities and their relationships. editor@iaeme.com https://iaeme.com/Home/journal/IJRCAIT 100 Graphrag and Role of Graph Databases in Advancing AI Graph Data Preprocessing: • Clean and normalize the graph data to ensure consistency and accuracy. • Perform entity linking and relation extraction to enrich the graph with semantic information. Indexing Methods: • Graph Indexing: Preserves the entire graph structure for efficient traversal and complex query processing. • Text Indexing: Converts graph elements into textual descriptions for seamless integration with LLMs. • Vector Indexing: Transforms graph data into vector representations for similarity-based retrieval. • Hybrid Indexing: Combines multiple methods for comprehensive and adaptable retrieval. [5,6] 2. Graph-Guided Retrieval Once the graph is indexed, GraphRAG employs intelligent retrieval mechanisms to pinpoint the most relevant information in response to user queries. This stage encompasses: Query Processing: • Analyze and expand user queries to improve retrieval accuracy and capture nuanced intent. • Implement query decomposition to break down complex questions into manageable sub-queries. Retriever Selection: • Non-parametric Retrievers: Utilize rule-based or traditional graph search algorithms for deterministic retrieval. • LM-based Retrievers: Leverage the semantic understanding of language models for context-aware retrieval.[5,6]  • GNN-based Retrievers: Employ graph neural networks to capture structural patterns and dependencies within the graph. Retrieval Paradigms: • Once Retrieval: Perform a single-pass retrieval for efficiency. • Iterative Retrieval: Refine retrieval results through multiple iterations for improved accuracy. • Multi-Stage Retrieval: Combine different retrieval methods in a pipeline for enhanced flexibility. Retrieval Granularity & Enhancement: • Support retrieval at various levels of granularity: nodes, triplets (subject-predicateobject), paths, subgraphs, or hybrid combinations. • Employ query enhancement (e.g., query expansion) and knowledge enhancement (e.g., merging and pruning retrieved information) techniques to further refine the retrieval process. https://iaeme.com/Home/journal/IJRCAIT 101 editor@iaeme.com Ravi Kiran Magham 3. Graph-Enhanced Generation The retrieved graph data is then transformed into a format that LLMs can readily understand, and the generation process is augmented to produce informative and contextually relevant responses.  This stage involves: Graph Format Conversion: • Convert the retrieved graph data into LLM-compatible formats, such as adjacency tables, natural language descriptions, or code-like representations. • Explore various graph languages and representations to optimize the integration with LLMs. Generator Selection: • GNNs: Leverage graph neural networks for tasks that require explicit reasoning over the graph structure. • LMs: Utilize language models for natural language generation and response formulation. • Hybrid Models: Combine GNNs and LMs in cascaded or parallel paradigms to leverage the strengths of both. Generation Enhancement: • Pre-Generation: Refine the input to the LLM before generation. • Mid-Generation: Guide the generation process using techniques like constrained decoding. • Post-Generation: Refine and combine the generated outputs for improved coherence and relevance. 4. Training and Optimization To achieve peak performance, GraphRAG systems undergo training and optimization procedures that fine-tune both the retrieval and generation components. This can involve: Retriever Training: • Explore training-free approaches for closed-source LLMs. • Implement training-based methods for custom retrievers using techniques like distant supervision and self-supervised pre-training. [5] Generator Training: • Adapt training strategies based on the chosen generator type (GNN, LM, or hybrid). • Implement supervised fine-tuning for generative LLMs to align them with the specific task and domain. Joint Training: Develop techniques for end-to-end optimization of both retrieval and generation components to ensure seamless collaboration. editor@iaeme.com https://iaeme.com/Home/journal/IJRCAIT 102 Graphrag and Role of Graph Databases in Advancing AI 5. Evaluation and Monitoring Rigorous evaluation and continuous monitoring are essential to ensure the effectiveness and reliability of GraphRAG systems. This involves: Task-Specific Metrics: • Implement relevant evaluation metrics tailored to the specific downstream tasks (e.g., Exact Match, F1 score for knowledge base question answering). Retrieval Quality Metrics: • Assess the relevance and coverage of the retrieved information. • Evaluate the consistency and reliability of retrieval results. Generation Quality Metrics: • Evaluate the accuracy, coherence, and fluency of the generated outputs. • Utilize standard metrics like BLEU, ROUGE-L, and METEOR for natural language generation tasks. This elaborated reference architecture provides a comprehensive framework for building GraphRAG systems, incorporating the latest advancements in graph-based retrieval and generation techniques as discussed in the cited literature. It allows for flexibility in component selection and optimization based on specific use cases and requirements, while addressing key challenges in areas such as scalability, efficiency, and domain adaptation. Fig 2: GraphRAG Architecture: Relative Significance of Core Elements [4-6] III. BENEFITS OF GRAPHRAG GraphRAG offers significant advantages over traditional Retrieval-Augmented Generation (RAG) systems, enhancing AI capabilities in several key areas: https://iaeme.com/Home/journal/IJRCAIT 103 editor@iaeme.com Ravi Kiran Magham 1. Improved accuracy and contextual relevance of responses: By leveraging knowledge graphs, GraphRAG provides LLMs with a more structured and comprehensive context. This results in responses that are not only more accurate but also more contextually appropriate, as the system can better understand the relationships between different pieces of information. Hu et al. [7] demonstrated that GraphRAG systems achieved a 15% improvement in response accuracy compared to traditional RAG methods when tested on complex question-answering tasks. 2. Better handling of complex, interconnected information: The graph-based approach allows GraphRAG to efficiently navigate and utilize intricate relationships between entities and concepts. This is particularly beneficial in domains with highly interconnected data, such as scientific research or complex business environments, where traditional RAG systems might struggle to capture the full context. Zhang et al. [8] showed that GraphRAG outperformed conventional RAG systems by 20% in tasks involving multi-hop reasoning on biomedical knowledge graphs. 3. Enhanced ability to answer global questions about large datasets: GraphRAG excels at synthesizing information from across large knowledge domains. Its graph-based structure allows for efficient traversal of vast datasets, enabling the system to answer broad, overarching questions that might require integrating information from multiple sources or domains. This capability is particularly valuable for tasks such as trend analysis and pattern recognition in large-scale data. 4. Reduced token usage and improved scalability: By storing information in a structured graph format, GraphRAG can more efficiently retrieve relevant context without needing to process large amounts of raw text. This leads to reduced token usage when generating prompts for LLMs, resulting in faster processing times and lower computational costs. Additionally, the graph structure allows for easier scaling as the knowledge base grows, maintaining performance even with large datasets. 5. Dynamic knowledge integration: Unlike traditional RAG systems that often work with static document collections, GraphRAG can easily incorporate new information into its knowledge graph. This allows the system to stay up-to-date with the latest information without requiring a complete reindexing of the dataset. This dynamic approach is particularly valuable in rapidly evolving fields such as current events analysis or technological research. 6. Improved reasoning capabilities: The graph structure enables more sophisticated reasoning, allowing GraphRAG to make inferences and connections that might not be immediately apparent in a flat text representation. This leads to more insightful and nuanced responses, particularly for complex queries. For instance, GraphRAG systems have shown superior performance in tasks requiring causal reasoning and relationship inference compared to traditional RAG methods [8]. These benefits make GraphRAG a powerful tool for enhancing AI systems' ability to work with domain-specific knowledge, handle complex queries, and provide more accurate and contextually relevant responses. As organizations increasingly deal with large, interconnected datasets, GraphRAG's capabilities offer a significant advantage over traditional RAG systems in developing more sophisticated and effective AI applications. IV. GRAPH DATABASES: FOUNDATIONS FOR ADVANCED AI Graph databases have emerged as a crucial technology in the AI landscape, offering unique capabilities for storing and managing highly connected data. Unlike traditional relational databases, graph databases are specifically designed to efficiently handle complex relationships between entities, making them ideal for supporting advanced AI applications like GraphRAG. editor@iaeme.com https://iaeme.com/Home/journal/IJRCAIT 104 Graphrag and Role of Graph Databases in Advancing AI A. Fundamentals of Graph Databases Graph databases are optimized for storing and querying interconnected data. They represent information as nodes (entities) and edges (relationships), allowing for efficient traversal and analysis of complex data structures. The property graph model, which is widely used in graph databases, extends this basic structure by allowing properties to be attached to both nodes and edges, providing a rich and flexible way to represent real-world data [9]. This model offers several advantages: 1. Intuitive representation: The node-edge structure closely mirrors how humans conceptualize relationships, making it easier to model and understand complex domains. 2. Performance optimization: By directly storing relationships, graph databases can perform queries that would be computationally expensive in relational databases much more efficiently. 3. Scalability: Graph databases can handle large-scale, highly connected data without significant performance degradation, making them suitable for big data applications. B. Key features of graph databases include: 1. Native graph storage: Data is stored in a graph structure, eliminating the need for complex joins and improving query performance. This approach allows for constanttime traversals per relationship, regardless of the total size of the graph. In large-scale AI applications, this can lead to orders of magnitude improvement in query performance compared to relational databases [9]. 2. Index-free adjacency: Relationships are physically stored as connections between nodes, enabling rapid traversal of the graph. This feature is particularly beneficial for applications that require real-time analysis of complex relationships, such as recommendation systems or fraud detection algorithms. 3. Flexible schema: Graph databases can easily adapt to changing data requirements without the need for extensive restructuring. This flexibility is crucial in AI applications where data models may evolve as new insights are gained or as the application's scope expands. 4. Powerful query languages: Graph-specific query languages like Cypher and Gremlin allow for expressive and efficient querying of graph data. These languages are designed to express complex patterns and traversals in a more intuitive and compact manner than traditional SQL, making it easier for developers to implement sophisticated AI algorithms [9]. Additional features that make graph databases particularly well-suited for handling complex, interconnected data in real-time applications include: 1. Native graph processing: Many graph databases offer built-in algorithms for common graph operations like shortest path finding, centrality calculation, and community detection. These can be leveraged directly in AI applications without the need for separate processing engines. 2. Multi-model capabilities: Some modern graph databases support multiple data models (e.g., document, key-value) alongside the graph model, allowing for greater flexibility in data representation and querying. 3. Distributed processing: To handle large-scale graphs, many graph databases offer distributed storage and processing capabilities, enabling them to scale horizontally across multiple machines. https://iaeme.com/Home/journal/IJRCAIT 105 editor@iaeme.com Ravi Kiran Magham Feature Graph Databases (Score 1-10) Complex relationship handling 10 Relational Databases (Score 1-10) Query performance for connected data 5 9 Scalability for large datasets 9 6 Intuitive data representation 7 9 Flexibility of schema 6 8 Native graph processing 5 10 Powerful query languages for graph operations 3 9 Index-free adjacency 9 5 Multi-model capabilities 3 7 Distributed processing 5 8 Table 2: Comparative Analysis: Graph Databases vs. Relational Databases for AI Applications [9] 7 V. THE SYNERGY BETWEEN GRAPHRAG AND GRAPH DATABASES The effectiveness of GraphRAG is closely tied to the capabilities of graph databases. This synergy manifests in several ways, enhancing AI systems' overall performance and capabilities that leverage both technologies. 1. Efficient Storage and Retrieval Graph databases provide the ideal infrastructure for storing and querying the knowledge graphs used in GraphRAG. Their optimized structure allows for rapid retrieval of relevant information when generating enhanced prompts for LLMs [10]. This efficiency is crucial for real-time applications where response time is critical. The native graph storage model employed by graph databases enables direct traversal of relationships between entities, eliminating the need for expensive join operations typically required in relational databases. This approach significantly reduces query complexity and improves retrieval speed, especially for highly interconnected data. Furthermore, graph databases often implement advanced indexing techniques specifically designed for graph structures. These indexes can dramatically improve the performance of common graph operations, such as neighborhood searches or path finding, which are essential for GraphRAG's context retrieval process. 2. Complex Query Processing Graph databases' ability to efficiently process complex queries enables GraphRAG to analyze relationships between entities and concepts quickly. This capability is crucial for generating comprehensive and context-aware prompts [10]. Graph databases excel at handling queries that involve multiple hops or complex patterns, which are common in knowledge graph exploration. Many graph databases support specialized query languages, such as Cypher or SPARQL, designed specifically for graph traversal and pattern matching. These languages allow for expressive and intuitive formulation of complex queries that would be cumbersome or inefficient to express in traditional SQL. Additionally, graph databases often implement advanced query optimization techniques tailored for graph structures. These optimizations can include intelligent query planning, adaptive runtime strategies, and caching mechanisms that further enhance the performance of complex queries. https://iaeme.com/Home/journal/IJRCAIT 106 editor@iaeme.com Graphrag and Role of Graph Databases in Advancing AI 3. Scalability and Performance As knowledge graphs grow in size and complexity, graph databases offer the scalability and performance necessary to maintain GraphRAG's effectiveness in real-world applications [10]. Modern graph databases are designed to handle billions of nodes and relationships, ensuring that even large-scale knowledge domains can be efficiently managed. To achieve this scalability, graph databases employ various strategies: • Distributed Storage: Many graph databases support horizontal scaling, partitioning the graph across multiple machines. This enables the system to handle graphs that are too large to fit on a single server. • Parallel Processing: Advanced graph databases leverage parallel processing techniques to distribute query workloads across multiple cores or machines, significantly improving performance for large-scale graphs. • Caching and In-memory Processing: Some graph databases offer in-memory processing capabilities or intelligent caching mechanisms to speed up frequently accessed parts of the graph. • Adaptive Indexing: As the graph evolves, some databases can automatically adjust their indexing strategies to maintain optimal performance. These scalability features ensure that GraphRAG can continue to operate efficiently as knowledge graphs expand, maintaining low latency and high throughput even for large and complex datasets. 1. Context Preservation: Graph databases excel at preserving the context of data relationships, which is essential for GraphRAG's operation. By maintaining the intricate connections between entities, graph databases allow GraphRAG to generate more informative and contextually relevant prompts. This context preservation enables the system to understand and utilize complex relationships that might be lost in traditional data storage systems. 2. Multi-hop Reasoning Capabilities: The combination of GraphRAG and graph databases significantly enhances multi-hop reasoning capabilities. Graph databases can efficiently perform queries that involve multiple steps or \"hops\" between entities, which is crucial for complex reasoning tasks. GraphRAG can leverage this capability to explore indirect relationships and draw insights that would be difficult or impossible with traditional data storage and retrieval methods. Capability GraphRAG Contribution Efficient Storage and Retrieval Graph Database Contribution Knowledge Graph Creation Context Retrieval Native Graph Storage Advanced Indexing Complex Query Processing Relationship Analysis Context-Aware Prompts Specialized Query Languages Query Optimization Techniques Scalability Dynamic Information Updates Large-Scale Knowledge Domains Distributed Storage Parallel Processing Caching and In-memory Processing Adaptive Indexing Real-time Performance Prompt Generation Semantic Reasoning Context Understanding Low Latency Queries Pattern Matching Table 2: Performance Metrics of Integrated GraphRAG and Graph Database Systems [10] https://iaeme.com/Home/journal/IJRCAIT 107 editor@iaeme.com Ravi Kiran Magham The combination of efficient storage and retrieval, powerful query processing, and robust scalability provided by graph databases creates a strong foundation for GraphRAG. This synergy enables the development of more sophisticated AI systems capable of reasoning over vast amounts of interconnected data, opening up new possibilities for applications in fields such as personalized medicine, complex system modeling, and advanced decision support systems. VI. CHALLENGES AND FUTURE DIRECTIONS While GraphRAG demonstrates significant potential, several challenges and areas for future research remain: 1. Scalability: As knowledge graphs expand, efficient retrieval and reasoning become increasingly challenging. The development of scalable algorithms for graph operations is crucial. Recent work by Galkin et al. [11] has proposed a foundation model for knowledge graph reasoning, which shows promise in handling large-scale graphs. However, further research is needed to address the exponential growth of potential subgraphs in massive datasets. Future work should focus on developing advanced indexing techniques and parallel processing methods to maintain performance as graph sizes increase. 2. Knowledge Graph Quality: The performance of GraphRAG systems is heavily dependent on the quality and completeness of the underlying knowledge graph. Techniques for automatic knowledge graph construction and maintenance are important areas of research. Zhu et al. [12] have explored using large language models for knowledge graph construction and reasoning, which shows potential for improving graph quality. However, ensuring consistency and accuracy in automatically constructed graphs remains a significant challenge. Future research should explore methods for continuous validation and refinement of knowledge graphs, potentially leveraging user feedback and multi-source information integration. 3. Integration with Continuous Learning: Exploring ways to update both the knowledge graph and the LLM components of GraphRAG in real-time could lead to more adaptive and up-to-date systems. This is particularly important in domains with rapidly changing information, such as current events or scientific research. Future work should investigate techniques for efficient incremental learning and graph updates while maintaining system coherence and performance. This might involve developing methods for dynamic knowledge integration and real-time model fine-tuning. 4. Multimodal GraphRAG: Extending the GraphRAG approach to include non-textual data (e.g., images, videos) in the knowledge graph could open up new possibilities for multimodal reasoning. This would enable more comprehensive and context-rich information retrieval and generation. While some initial work has been done on multimodal knowledge graphs, integrating these with large language models for retrieval and generation tasks remains an open challenge. Future research should focus on developing unified representations for diverse data types and designing retrieval mechanisms that can effectively leverage multimodal information. Addressing these challenges will be crucial for advancing the field of GraphRAG and realizing its full potential across various applications. As research progresses, we can expect to see more robust, scalable, and versatile GraphRAG systems that can handle increasingly complex reasoning tasks across diverse domains and data types. CONCLUSION The integration of GraphRAG and graph databases represents a significant advancement in AI's ability to reason with complex, interconnected information. This synergy enables the development of more sophisticated AI systems capable of understanding and utilizing vast amounts of structured and unstructured data.  editor@iaeme.com https://iaeme.com/Home/journal/IJRCAIT 108 Graphrag and Role of Graph Databases in Advancing AI By leveraging the strengths of graph-based knowledge representation and advanced language models, GraphRAG overcomes limitations of traditional RAG systems, offering improved accuracy, contextual relevance, and scalability. As research in this field progresses, we can anticipate more robust and versatile AI applications across diverse domains such as personalized medicine, complex system modeling, and advanced decision support systems. While challenges remain, particularly in scalability and knowledge graph quality, the potential of GraphRAG to push the boundaries of AI reasoning and real-world applications is substantial. As these technologies continue to evolve, they promise to drive further advancements in artificial intelligence, opening new possibilities for solving complex problems and gaining insights from interconnected data. REFERENCES [1] [2]  [3] [4] [5] [6] [7] T. L. Scao et al., \"Bloom: A 176B-Parameter Open-Access Multilingual Language Model,\" arXiv preprint arXiv:2211.05100, 2022. [Online]. Available: https://arxiv.org/abs/2211.05100 G. Izacard, P. Lewis, E. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi, A. Cancedda, S. Riedel, and S. Stenetorp, \"Atlas: Few-shot Learning with Retrieval Augmented Language Models,\" arXiv preprint arXiv:2208.03299, 2022. [Online]. Available: https://arxiv.org/abs/2208.03299 Robinson, J. Webber, and E. Eifrem, \"Graph Databases: New Opportunities for Connected Data,\" 3rd Edition, O'Reilly Media, Inc., 2021. [Online]. Available: https://www.oreilly.com/library/view/graph-databases/9781492044062/ Sara AlMahri, Liming Xu, Alexandra Brintrup. \"Enhancing Supply Chain Visibility with Knowledge Graphs and Large Language Models.\" arXiv:2408.07705v1 [cs.IR], Aug 2024. https://arxiv.org/html/2408.07705v1 Boci Peng, Yun Zhu, et al. \"Graph Retrieval-Augmented Generation: A Survey.\" arXiv:2408.08921, Sep 2024. https://arxiv.org/pdf/2408.08921 Ben Lorica, Prashanth Rao. \"GraphRAG: Design Patterns, Challenges, Recommendations.\" Gradient Flow, May 2024. https://gradientflow.com/graphragdesign-patterns-challenges-recommendations/ Y. Hu, Z. Zhang, Y. Lei, G. Pan, C. Ling, and L. Zhao, \"GRAG: Graph RetrievalAugmented Generation,\" in 2024 IEEE International Conference on Data Engineering (ICDE), 2024, https://ieeexplore.ieee.org/document/10409137 [8] pp. 1-12. J. Zhang, X. Zhang, J. Yu, J. Tang, J. Tang, C. Li, and H. Chen, \"Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering,\" in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), https://aclanthology.org/2022.acl-long.396/ [9] O'Reilly 2022, pp. 5773-5784. J. Webber and I. Robinson, \"Graph Databases: New Opportunities for Connected Data,\" Media, Inc., 2nd Edition, https://www.oreilly.com/library/view/graph-databases-new/9781098155308/ https://iaeme.com/Home/journal/IJRCAIT 109 2021. editor@iaeme.com Ravi Kiran Magham [10] [11] [12] Data,\" I. Robinson, J. Webber, and E. Eifrem, \"Graph Databases: New Opportunities for Connected O'Reilly Media, Inc., 3rd Edition, https://www.oreilly.com/library/view/graph-databases-3rd/9781098150013/ 2023. M. Galkin, X. Yuan, H. Mostafa, J. Tang, and Z. Zhu, \"Towards Foundation Models for Knowledge Graph Reasoning,\" in The Twelfth International Conference on Learning Representations, 2024. https://openreview.net/forum?id=V8N9bxJAh8 Y. Zhu, X. Wang, J. Chen, S. Qiao, Y. Ou, Y. Yao, S. Deng, H. Chen, and N. Zhang, \"LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities,\" arXiv:2305.13168 https://arxiv.org/abs/2305.13168 [cs.AI], May 2023. Citation: Ravi Kiran Magham. (2024). Graphrag and Role of Graph Databases in Advancing AI. International Journal of Research in Computer Applications and Information Technology (IJRCAIT), 7(2), 98-110. Abstract Link: https://iaeme.com/Home/article_id/IJRCAIT_07_02_007 Article Link:  https://iaeme.com/MasterAdmin/Journal_uploads/IJRCAIT/VOLUME_7_ISSUE_2/IJRCAIT_07_02_007.pdf ✉ editor@iaeme.com  \"\"\"\n",
        "kg = KnowledgeGraph(client, claude_model)\n",
        "\n",
        "await kg.process_large_document(document_text)\n",
        "\n",
        "\n",
        "analysis = kg.analyze_graph()\n",
        "\n",
        "print(analysis)\n",
        "\n",
        "graph_data = kg.export_graph()\n",
        "\n",
        "kg.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhuwMBxf_LGx",
        "outputId": "955e8053-b1f4-41d2-fe7d-7ab452251939"
      },
      "id": "YhuwMBxf_LGx",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting document into chunks...\n",
            "Document split into 9 chunks\n",
            "Processing batch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: RELATED_TO)} {position: line: 2, column: 49, offset: 49} for query: '\\n            MATCH (s:Entity {name: $source})-[r:RELATED_TO]-(t:Entity {name: $target})\\n            RETURN r\\n            '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 2/2\n",
            "Extraction complete! Found 37 entities and 52 relationships\n",
            "{'total_entities': 37, 'total_relationships': 40, 'most_connected_entities': [{'entity': 'Graphrag', 'type': 'Technology', 'degree': 23}, {'entity': 'Graph Databases', 'type': 'Technology', 'degree': 14}, {'entity': 'Large Language Models', 'type': 'Technology', 'degree': 4}, {'entity': 'Knowledge Graphs', 'type': 'Concept', 'degree': 3}, {'entity': 'Artificial Intelligence', 'type': 'Concept', 'degree': 2}, {'entity': 'Retrieval-Augmented Generation', 'type': 'Methodology', 'degree': 2}, {'entity': 'Knowledge Graph', 'type': 'Concept', 'degree': 2}, {'entity': 'Large Language Models (Llms)', 'type': 'Technology', 'degree': 1}, {'entity': 'Query-Focused Summarization', 'type': 'Methodology', 'degree': 1}, {'entity': 'Graph-Guided Retrieval', 'type': 'Methodology', 'degree': 1}], 'entity_types_distribution': {'Technology': 15, 'Concept': 10, 'Methodology': 7, 'Metric': 3, 'Tool': 2}, 'average_degree': 2.1621621621621623}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "graphrag",
      "language": "python",
      "name": "graphrag"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}